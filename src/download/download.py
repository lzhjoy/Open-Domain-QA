# åå°è¿è¡Œå‘½ä»¤: nohup python ./src/download/download.py > logs/rmrb_downloader.log 2>&1 & 

import requests
import bs4
import os
import sys
import datetime
import time
import json
from tqdm import tqdm

ODQA_ROOT_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, ODQA_ROOT_PATH)

class RMRBDownloader:
    def __init__(self, begin_date=None, end_date=None, dest_dir="./data/PeoplesDaily"):
        """
        äººæ°‘æ—¥æŠ¥çˆ¬è™«ç±»åˆå§‹åŒ–
        :param begin_date: å¼€å§‹æ—¥æœŸ(æ ¼å¼å¦‚20220706)
        :param end_date: ç»“æŸæ—¥æœŸ(æ ¼å¼å¦‚20220706)
        :param dest_dir: ä¿å­˜æ–‡ä»¶çš„ç›®å½•
        """
        self.begin_date = begin_date
        self.end_date = end_date
        self.dest_dir = dest_dir
        self.data_dict = {}
        
        # æ·»åŠ é‡è¯•é…ç½®
        self.max_retries = 3
        self.retry_delay = 0  # ç§’

    def _fetch_url(self, url, retries=None):
        """
        è·å–ç½‘é¡µå†…å®¹ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
        :param url: è¦è·å–çš„URL
        :param retries: å½“å‰é‡è¯•æ¬¡æ•°
        :return: ç½‘é¡µå†…å®¹æˆ–None
        """
        if retries is None:
            retries = self.max_retries
            
        headers = {
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
        }

        try:
            r = requests.get(url, headers=headers, timeout=10)  # æ·»åŠ è¶…æ—¶è®¾ç½®
            
            # æ£€æŸ¥çŠ¶æ€ç 
            if r.status_code == 404:
                return None
                
            r.raise_for_status()
            r.encoding = r.apparent_encoding
            return r.text
            
        except requests.exceptions.Timeout:
            if retries > 0:
                time.sleep(self.retry_delay)
                return self._fetch_url(url, retries - 1)
            else:
                return None
                
        except requests.exceptions.HTTPError as e:
            if retries > 0 and e.response.status_code >= 500:  # åªæœ‰æœåŠ¡å™¨é”™è¯¯æ‰é‡è¯•
                time.sleep(self.retry_delay)
                return self._fetch_url(url, retries - 1)
            else:
                return None
                
        except requests.exceptions.ConnectionError:
            if retries > 0:
                time.sleep(self.retry_delay)
                return self._fetch_url(url, retries - 1)
            else:
                return None
                
        except requests.exceptions.RequestException:
            return None

    def _get_page_list(self, year, month, day):
        """
        è·å–ç‰ˆé¢åˆ—è¡¨
        :return: ç‰ˆé¢URLåˆ—è¡¨
        """
        url = f'http://paper.people.com.cn/rmrb/html/{year}-{month}/{day}/nbs.D110000renmrb_01.htm'
        print(f"ğŸ” æ­£åœ¨è·å–ç‰ˆé¢åˆ—è¡¨: {year}-{month}-{day}")
        
        html = self._fetch_url(url)
        if html is None:
            print(f"âŒ æ—¥æœŸ {year}-{month}-{day} çš„ç‰ˆé¢åˆ—è¡¨è·å–å¤±è´¥ï¼Œå¯èƒ½è¯¥æ—¥æŠ¥çº¸ä¸å­˜åœ¨")
            return []
            
        try:
            bsobj = bs4.BeautifulSoup(html, 'html.parser')
            temp = bsobj.find('div', attrs={'id': 'pageList'})
            if temp:
                pageList = temp.ul.find_all('div', attrs={'class': 'right_title-name'})
            else:
                pageList = bsobj.find('div', attrs={
                                    'class': 'swiper-container'}).find_all('div', attrs={'class': 'swiper-slide'})
                                    
            if not pageList:
                print(f"âš ï¸ æ—¥æœŸ {year}-{month}-{day} çš„ç‰ˆé¢åˆ—è¡¨ä¸ºç©ºï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²æ”¹å˜")
                return []
                
            linkList = []
            for page in pageList:
                try:
                    link = page.a["href"]
                    url = f'http://paper.people.com.cn/rmrb/html/{year}-{month}/{day}/{link}'
                    linkList.append(url)
                except (AttributeError, KeyError):
                    continue
            
            print(f"âœ… æ—¥æœŸ {year}-{month}-{day} æˆåŠŸè·å– {len(linkList)} ä¸ªç‰ˆé¢")
            return linkList
            
        except Exception as e:
            print(f"âŒ è§£æç‰ˆé¢åˆ—è¡¨å¼‚å¸¸: {year}-{month}-{day} - {str(e)}")
            return []

    def _get_title_list(self, year, month, day, page_url):
        """
        è·å–æ–‡ç« åˆ—è¡¨
        :return: æ–‡ç« URLåˆ—è¡¨
        """
        print(f"ğŸ“‹ æ­£åœ¨è·å–ç‰ˆé¢æ–‡ç« : {page_url.split('/')[-1]}")
        
        html = self._fetch_url(page_url)
        if html is None:
            print(f"âŒ ç‰ˆé¢æ–‡ç« åˆ—è¡¨è·å–å¤±è´¥: {page_url}")
            return []
            
        try:
            bsobj = bs4.BeautifulSoup(html, 'html.parser')
            temp = bsobj.find('div', attrs={'id': 'titleList'})
            if temp:
                titleList = temp.ul.find_all('li')
            else:
                titleList = bsobj.find(
                    'ul', attrs={'class': 'news-list'}).find_all('li')
                    
            if not titleList:
                print(f"âš ï¸ ç‰ˆé¢æ–‡ç« åˆ—è¡¨ä¸ºç©º: {page_url}")
                return []
                
            linkList = []
            for title in titleList:
                try:
                    tempList = title.find_all('a')
                    for temp in tempList:
                        link = temp["href"]
                        if 'nw.D110000renmrb' in link:
                            url = f'http://paper.people.com.cn/rmrb/html/{year}-{month}/{day}/{link}'
                            linkList.append(url)
                except (AttributeError, KeyError):
                    continue
            
            print(f"âœ… ç‰ˆé¢ {page_url.split('/')[-1]} æˆåŠŸè·å– {len(linkList)} ç¯‡æ–‡ç« é“¾æ¥")
            return linkList
            
        except Exception as e:
            print(f"âŒ è§£ææ–‡ç« åˆ—è¡¨å¼‚å¸¸: {page_url} - {str(e)}")
            return []

    def _get_content(self, html, url):
        """
        è·å–æ–‡ç« å†…å®¹
        :return: æ–‡ç« å†…å®¹å­—å…¸æˆ–None
        """
        if not html:
            print(f"âŒ è·å–å¤±è´¥: {url} (æ— HTMLå†…å®¹)")
            return None
            
        print(f"ğŸ“„ æ­£åœ¨è§£æ: {url}")    
        try:
            bsobj = bs4.BeautifulSoup(html, 'html.parser')
            
            # è·å–æ ‡é¢˜ï¼Œå¤„ç†å¯èƒ½ç¼ºå¤±çš„æƒ…å†µ
            title_parts = []
            for title_tag in ['h3', 'h1', 'h2']:
                try:
                    tag = bsobj.find(title_tag)
                    if tag:
                        title_parts.append(tag.text)
                except:
                    pass
                    
            if not title_parts:
                title = "æ— æ ‡é¢˜"
                print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æ ‡é¢˜ - {url}")
            else:
                title = '\n'.join(title_parts)
                
            # è·å–å†…å®¹
            content_div = bsobj.find('div', attrs={'id': 'ozoom'})
            if not content_div:
                print(f"âŒ è·å–å¤±è´¥: {url} (æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸ)")
                return None
                
            pList = content_div.find_all('p')
            if not pList:
                print(f"âŒ è·å–å¤±è´¥: {url} (å†…å®¹ä¸ºç©º)")
                return None
                
            content = ''
            for p in pList:
                content += p.text + '\n'
                
            content_stripped = content.strip()
            title_stripped = title.strip()
            resp = {"url": url, "title": title_stripped, "content": content_stripped}
            
            # æ‰“å°æˆåŠŸä¿¡æ¯ï¼Œæ˜¾ç¤ºæ ‡é¢˜å’Œå†…å®¹é•¿åº¦
            print(f"âœ… æˆåŠŸè·å–: [{title_stripped[:20]}{'...' if len(title_stripped) > 20 else ''}] ({len(content_stripped)} å­—ç¬¦)")
            return resp
            
        except AttributeError as e:
            print(f"âŒ è§£æé”™è¯¯: {url} - å±æ€§é”™è¯¯: {str(e)}")
            return None
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {url} - {str(e)}")
            return None

    def _save_json_file(self, data, path, filename):
        """
        ä¿å­˜JSONæ–‡ä»¶
        """
        if not data:
            return False
            
        try:
            if not os.path.exists(path):
                os.makedirs(path)
                
            with open(os.path.join(path, filename), 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
                
            return True
            
        except IOError:
            return False
        except Exception:
            return False

    def _download_rmrb(self, year, month, day):
        """
        ä¸‹è½½æŒ‡å®šæ—¥æœŸçš„äººæ°‘æ—¥æŠ¥æ–‡ç« 
        """
        print(f"ğŸ“… å¼€å§‹ä¸‹è½½ {year}-{month}-{day} çš„æ–‡ç« ")
        
        pageList = self._get_page_list(year, month, day)
        if not pageList:
            print(f"ğŸ“­ æ—¥æœŸ {year}-{month}-{day} æ²¡æœ‰å¯ç”¨ç‰ˆé¢ï¼Œè·³è¿‡")
            return 0
            
        article_count = 0
        downloaded_count = 0
        daily_articles = []
        
        for i, page in enumerate(pageList, 1):
            print(f"â³ å¤„ç†ç¬¬ {i}/{len(pageList)} ä¸ªç‰ˆé¢: {page.split('/')[-1]}")
            titleList = self._get_title_list(year, month, day, page)
            article_count += len(titleList)
            
            if not titleList:
                continue
                
            for j, url in enumerate(titleList, 1):
                print(f"  â³ å¤„ç†ç¬¬ {j}/{len(titleList)} ç¯‡æ–‡ç« ...")
                html = self._fetch_url(url)
                if html is None:
                    continue
                    
                content = self._get_content(html, url)
                if content is None:
                    continue
                
                daily_articles.append(content)
                downloaded_count += 1
                
                # æ·»åŠ çˆ¬å–å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                # time.sleep(0.5)
        
        # å°†å½“å¤©æ–‡ç« ä¿å­˜åˆ°æ•°æ®å­—å…¸
        key = f'{year}{month}'
        if key not in self.data_dict:
            self.data_dict[key] = []
        self.data_dict[key].extend(daily_articles)
        
        success_rate = 0 if article_count == 0 else (downloaded_count / article_count) * 100
        print(f"ğŸ“Š æ—¥æœŸ {year}-{month}-{day} ç»Ÿè®¡: {downloaded_count}/{article_count} ç¯‡æ–‡ç« ä¸‹è½½æˆåŠŸ (æˆåŠŸç‡: {success_rate:.1f}%)")
        return downloaded_count

    def _gen_dates(self, b_date, days):
        """ç”Ÿæˆæ—¥æœŸåºåˆ—"""
        day = datetime.timedelta(days=1)
        for i in range(days):
            yield b_date + day * i

    def _get_date_list(self, begin_date, end_date):
        """è·å–æ—¥æœŸèŒƒå›´åˆ—è¡¨"""
        try:
            start = datetime.datetime.strptime(begin_date, "%Y%m%d")
            end = datetime.datetime.strptime(end_date, "%Y%m%d")
            
            if start > end:
                return []
                
            data = []
            for d in self._gen_dates(start, (end-start).days + 1):  # +1ç¡®ä¿åŒ…å«ç»“æŸæ—¥æœŸ
                data.append(d)
                
            return data
            
        except ValueError:
            return []
            
    def clean_json_files(self):
        for filename in tqdm(os.listdir(self.dest_dir)):
            if filename.endswith(".json"):
                file_path = os.path.join(self.dest_dir, filename)
                with open(file_path, 'r', encoding='utf-8') as file:
                    data = json.load(file)

                # ä¿ç•™ä¸ç¬¦åˆæ¡ä»¶çš„æ¡ç›®
                cleaned_data = [
                    item for item in data if 'æœ¬ç‰ˆè´£ç¼–' not in item['title'] and item['content'].strip() != '']

                with open(file_path, 'w', encoding='utf-8') as file:
                    json.dump(cleaned_data, file, ensure_ascii=False, indent=4)
                    
    def run(self):
        """
        è¿è¡Œçˆ¬è™«
        :return: æ–‡ç« æ•°æ®å­—å…¸
        """ 
        if not self.begin_date or not self.end_date:
            print("âŒ é”™è¯¯: å¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸä¸èƒ½ä¸ºç©º")
            return None
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å–äººæ°‘æ—¥æŠ¥æ–‡ç« ï¼Œæ—¥æœŸèŒƒå›´: {self.begin_date} è‡³ {self.end_date}")
        
        data = self._get_date_list(self.begin_date, self.end_date)
        if not data:
            print("âŒ é”™è¯¯: æ—¥æœŸåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­")
            return None
            
        print(f"ğŸ“† å…±æœ‰ {len(data)} å¤©éœ€è¦å¤„ç†")
        
        self.data_dict = {}
        total_articles = 0
        saved_files = 0
        success_days = 0
        empty_days = 0
        
        # æŒ‰æœˆåˆ†ç»„å¤„ç†
        current_month = None
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        for d in tqdm(data, desc="ğŸ“ˆ çˆ¬å–è¿›åº¦"):
            year = str(d.year)
            month = str(d.month) if d.month >= 10 else '0' + str(d.month)
            day = str(d.day) if d.day >= 10 else '0' + str(d.day)
            year_month = f"{year}{month}"
            
            # å¦‚æœæœˆä»½å˜åŒ–ï¼Œä¿å­˜å‰ä¸€ä¸ªæœˆçš„æ•°æ®
            if current_month and current_month != year_month and current_month in self.data_dict and self.data_dict[current_month]:
                print(f"\n{'='*50}")
                print(f"ğŸ’¾ ä¿å­˜ {current_month[:4]}-{current_month[4:]} æœˆæ•°æ®...")
                filename = f"{current_month[:4]}-{current_month[4:]}.json"
                
                if self._save_json_file(self.data_dict[current_month], self.dest_dir, filename):
                    saved_files += 1
                    article_count = len(self.data_dict[current_month])
                    print(f"âœ… å·²ä¿å­˜: {filename} ({article_count} ç¯‡æ–‡ç« )")
                    
                # æ¸…ç©ºå·²ä¿å­˜çš„æœˆä»½æ•°æ®ï¼Œå‡å°‘å†…å­˜å ç”¨
                self.data_dict[current_month] = []
            
            # æ›´æ–°å½“å‰æœˆä»½
            current_month = year_month
            
            print(f"\n{'='*50}")
            print(f"ğŸ“… å¼€å§‹å¤„ç† {year}-{month}-{day}")
            print(f"{'='*50}")
            
            try:
                articles = self._download_rmrb(year, month, day)
                total_articles += articles
                
                if articles > 0:
                    success_days += 1
                else:
                    empty_days += 1
                    
                print(f"âœ“ {year}-{month}-{day} å®Œæˆï¼Œè·å– {articles} ç¯‡æ–‡ç« ")
                print(f"{'='*50}\n")
                
            except Exception as e:
                print(f"âŒ å¤„ç† {year}-{month}-{day} æ—¶å‡ºé”™: {str(e)}")
                empty_days += 1
                print(f"{'='*50}\n")
                continue
        
        # ä¿å­˜æœ€åä¸€ä¸ªæœˆçš„æ•°æ®
        for key, value in self.data_dict.items():
            if not value:  # è·³è¿‡ç©ºæ•°æ®
                continue
                
            year, month = key[:4], key[4:]
            filename = f'{year}-{month}.json'
            
            print(f"\n{'='*50}")
            print(f"ğŸ’¾ ä¿å­˜ {year}-{month} æœˆæ•°æ®...")
            
            if self._save_json_file(value, self.dest_dir, filename):
                saved_files += 1
                print(f"âœ… å·²ä¿å­˜: {filename} ({len(value)} ç¯‡æ–‡ç« )")
        
        print(f"\n{'='*50}")
        print(f"ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"   ğŸ“‘ æ€»æ–‡ç« æ•°: {total_articles} ç¯‡")
        print(f"   ğŸ“ ä¿å­˜æ–‡ä»¶: {saved_files} ä¸ª")
        print(f"   ğŸ“… æˆåŠŸå¤©æ•°: {success_days}/{len(data)} å¤©")
        print(f"   ğŸ“­ ç©ºå†…å®¹å¤©æ•°: {empty_days}/{len(data)} å¤©")
        print(f"{'='*50}")
        return self.data_dict


if __name__ == '__main__':
    print("\n" + "="*60)
    print("ğŸ—ï¸  äººæ°‘æ—¥æŠ¥æ–‡ç« çˆ¬è™« v1.0")
    print("="*60)
    
    # ç¤ºä¾‹ç”¨æ³•
    dir = "./data/PeoplesDaily"
    if os.path.exists(dir):
        print("ğŸ§¹ æ¸…ç†ç°æœ‰JSONæ–‡ä»¶ä¸­...")
        downloader = RMRBDownloader('20230501', '20240430')
        downloader.clean_json_files()
        print("âœ… æ–‡ä»¶æ¸…ç†å®Œæˆï¼")
    else:
        print("ğŸ” å¼€å§‹çˆ¬å–äººæ°‘æ—¥æŠ¥æ–‡ç« ï¼Œæ—¥æœŸèŒƒå›´: 20230501 è‡³ 20240430")
        downloader = RMRBDownloader('20230501', '20240430')
        # downloader = RMRBDownloader('20230501', '20230502')
        downloader.run()
        print("ğŸ§¹ å¼€å§‹æ¸…ç†JSONæ–‡ä»¶...")
        downloader.clean_json_files()
        print("âœ… ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")
    
    print("="*60)